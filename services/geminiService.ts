import { GoogleGenAI, Modality } from "@google/genai";
import { NewsItem, ScriptLine, BroadcastSegment, VideoAssets, ViralMetadata, ChannelConfig, Scene, ScriptWithScenes, VideoMode, ShotType } from "../types";
import { ContentCache } from "./ContentCache";
import { retryWithBackoff, retryVideoGeneration } from "./retryUtils";
import { getModelForTask, getCostForTask } from "./modelStrategy";
import { CostTracker } from "./CostTracker";
import { 
  getChannelIntroOutro, 
  saveChannelIntroOutro,
  saveGeneratedVideo,
  findCachedVideo,
  findCachedVideoByDialogue,
  getCachedChannelVideos,
  createPromptHash,
  VideoType,
  VideoProvider,
  uploadAudioToStorage,
  // Pending video task management
  findPendingVideoTask,
  saveVideoTaskPending,
  updateVideoTaskCompleted,
  updateVideoTaskFailed,
  // Thumbnail cache
  findCachedThumbnail,
  saveThumbnailToCache,
  // Channel management
  getChannelById
} from "./supabaseService";
import { 
  createInfiniteTalkMultiTask,
  createInfiniteTalkSingleTask,
  pollInfiniteTalkTask,
  getSilentAudioUrl,
  createWavespeedImageTask, 
  pollWavespeedImageTask,
  checkWavespeedConfig 
} from "./wavespeedProxy";
import { getSeedImageForScene } from "./seedImageVariations";

// Import new services
import { 
  generateScriptWithGPT,
  generateViralMetadataWithGPT,
  generateViralHookWithGPT,
  generateTTSAudio,
  generateImageWithDALLE,
  checkOpenAIConfig,
  createTitleVariantFallback,
  analyzeScriptForShorts,
  openaiRequest // For local regenerateScene implementation
} from "./openaiService";
import type { ScriptAnalysis } from "./openaiService";
import { 
  generateElevenLabsTTS,
  checkElevenLabsConfig
} from "./elevenlabsService";
import { 
  fetchNewsWithSerpAPI,
  fetchTrendingWithSerpAPI,
  checkSerpAPIConfig
} from "./serpApiService";
import { 
  generateScenePrompts, 
  ScenePrompt, 
  SCENE_BUILDER_DEFAULTS 
} from "./sceneBuilderService";
import { 
  analyzeScriptRetention,
  validateScriptForVirality
} from "./scriptRetentionAnalyzer";
import { 
  ShotstackService, 
  checkShotstackConfig,
  createCompositionFromSegments,
  CompositionConfig,
  RenderResult,
  // Podcast-style composition (following shockstack.md guide)
  renderPodcastVideo,
  PodcastScene
} from "./shotstackService";

const getApiKey = () => import.meta.env.VITE_GEMINI_API_KEY || window.env?.API_KEY || process.env.API_KEY || "";
const getAiClient = () => new GoogleGenAI({ apiKey: getApiKey() });

// Helper function to check if Wavespeed is configured (via proxy or direct API key)
const isWavespeedConfigured = () => {
  const config = checkWavespeedConfig();
  return config.configured;
};

// Helper function to get seed image URL based on channel format (16:9 or 9:16)
const getSeedImageUrl = (
  config: ChannelConfig, 
  imageType: 'hostA' | 'hostB' | 'twoShot'
): string | undefined => {
  const isVertical = config.format === '9:16';
  const seedImages = config.seedImages;
  
  if (!seedImages) return undefined;
  
  if (imageType === 'hostA') {
    // Prefer format-specific image, fallback to other format if not available
    return isVertical 
      ? (seedImages.hostASoloUrl_9_16 || seedImages.hostASoloUrl)
      : (seedImages.hostASoloUrl || seedImages.hostASoloUrl_9_16);
  } else if (imageType === 'hostB') {
    return isVertical 
      ? (seedImages.hostBSoloUrl_9_16 || seedImages.hostBSoloUrl)
      : (seedImages.hostBSoloUrl || seedImages.hostBSoloUrl_9_16);
  } else {
    return isVertical 
      ? (seedImages.twoShotUrl_9_16 || seedImages.twoShotUrl)
      : (seedImages.twoShotUrl || seedImages.twoShotUrl_9_16);
  }
};

// Channel-specific branding overrides (intro/outro assets)
const CHANNEL_BRANDING_OVERRIDES: Record<string, { intro: string; outro: string }> = {
  chimpnews: {
    intro: "https://dbtlmnvcrsbrtruipvyg.supabase.co/storage/v1/object/public/channel-assets/videos/intro_outros/chimp_news_intro.mp4",
    outro: "https://dbtlmnvcrsbrtruipvyg.supabase.co/storage/v1/object/public/channel-assets/videos/intro_outros/chimp_news_outro.mp4"
  }
};

const normalizeChannelKey = (channelName?: string) =>
  (channelName || '').replace(/\s+/g, '').toLowerCase();

// Helper function to clean and parse JSON from Gemini responses
// Gemini sometimes returns control characters inside strings which breaks JSON.parse
const cleanAndParseGeminiJSON = <T>(text: string, fallback: T): T => {
  // Cleanup markdown code blocks and trim
  let cleanText = text.replace(/```json\n?/g, "").replace(/```/g, "").trim();
  
  // Fix control characters INSIDE JSON string values (common Gemini issue)
  // This regex finds string content between quotes and escapes control chars
  cleanText = cleanText.replace(/"([^"\\]*(\\.[^"\\]*)*)"/g, (match) => {
    return match.replace(/[\x00-\x1F\x7F]/g, (char) => {
      switch (char) {
        case '\n': return '\\n';
        case '\r': return '\\r';
        case '\t': return '\\t';
        default: return ''; // Remove other control characters
      }
    });
  });

  try {
    return JSON.parse(cleanText) as T;
  } catch (e) {
    console.error("JSON parsing error:", e);
    console.error("Cleaned text (first 300 chars):", cleanText.substring(0, 300));
    throw e;
  }
};

// Helper function to create a simple placeholder image as data URI
const createPlaceholderImage = (width: number = 1024, height: number = 1024): string => {
  if (typeof document === 'undefined') {
    return 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNk+M9QDwADhgGAWjR9awAAAABJRU5ErkJggg==';
  }
  
  const canvas = document.createElement('canvas');
  canvas.width = width;
  canvas.height = height;
  const ctx = canvas.getContext('2d');
  if (ctx) {
    const gradient = ctx.createLinearGradient(0, 0, width, height);
    gradient.addColorStop(0, '#1a1a1a');
    gradient.addColorStop(0.5, '#2a2a2a');
    gradient.addColorStop(1, '#1a1a1a');
    ctx.fillStyle = gradient;
    ctx.fillRect(0, 0, width, height);
    ctx.fillStyle = 'rgba(40, 40, 40, 0.3)';
    ctx.fillRect(width * 0.2, height * 0.2, width * 0.6, height * 0.6);
  }
  return canvas.toDataURL('image/png');
};

// =============================================================================================
// INFINITETALK VIDEO GENERATION (WaveSpeed Only - No VEO3)
// =============================================================================================

/**
 * InfiniteTalk pricing (per 5 seconds):
 * - 480p: $0.15
 * - 720p: $0.30
 */
const INFINITETALK_COST_480P = 0.15;
const INFINITETALK_COST_720P = 0.30;

interface InfiniteTalkVideoOptions {
  channelId: string;
  productionId?: string;
  segmentIndex: number;
  audioUrl: string;           // URL to the audio file (primary, or hostA for "both" scenes)
  referenceImageUrl: string;  // URL to the reference image
  speaker: string;            // Who is speaking: hostA name, hostB name, or "Both"
  dialogueText: string;       // The dialogue text for caching
  hostAName: string;          // Name of host A (left in image)
  hostBName: string;          // Name of host B (right in image)
  hostAVisualPrompt: string;  // Visual description of host A
  hostBVisualPrompt: string;  // Visual description of host B
  resolution?: '480p' | '720p';
  aspectRatio?: '16:9' | '9:16'; // Video aspect ratio based on channel format
  // For "both" scenes - separate audio URLs
  hostA_audioUrl?: string;    // Audio URL for Host A (left side)
  hostB_audioUrl?: string;    // Audio URL for Host B (right side)
  order?: 'left_first' | 'right_first' | 'meanwhile';  // Who speaks first
  // Scene metadata from v2.0 Narrative Engine
  sceneMetadata?: {
    video_mode: VideoMode;
    model: 'infinite_talk' | 'infinite_talk_multi';
    shot: ShotType;
    scenePrompt?: string; // Prompt from Scene Builder
  };
  // Duration from audio for accurate video timing
  audioDuration?: number; // Duration in seconds
}

/**
 * Generate a lip-sync video using WaveSpeed InfiniteTalk
 * 
 * MODEL SELECTION:
 * - InfiniteTalk Single: For scenes with ONE host (hostA or hostB alone)
 * - InfiniteTalk Multi: For scenes with BOTH hosts in frame
 * 
 * IMPORTANT: Uses strict prompts to maintain character consistency
 */
const generateInfiniteTalkVideo = async (
  options: InfiniteTalkVideoOptions
): Promise<{ url: string | null; fromCache: boolean; durationSeconds?: number }> => {
  const { 
    channelId, 
    productionId, 
    segmentIndex,
    audioUrl, 
    referenceImageUrl, 
    speaker,
    dialogueText,
    hostAName,
    hostBName,
    hostAVisualPrompt,
    hostBVisualPrompt,
    resolution = '720p',
    aspectRatio = '16:9',  // Default to landscape
    hostA_audioUrl,  // Separate audio for Host A (for "both" scenes)
    hostB_audioUrl,  // Separate audio for Host B (for "both" scenes)
    order: providedOrder,  // Who speaks first
    sceneMetadata,
    audioDuration  // Duration from audio for accurate timing
  } = options;

  // Determine video type and model based on scene metadata or speaker
  let videoType: VideoType = 'segment';
  let effectiveVideoMode: VideoMode | null = null;
  let useMultiModel: boolean = false;
  
  if (sceneMetadata?.video_mode) {
    // Convert legacy "both" to hostA for backwards compatibility
    effectiveVideoMode = (sceneMetadata.video_mode as string) === 'both' ? 'hostA' : sceneMetadata.video_mode;
    videoType = effectiveVideoMode === 'hostA' ? 'host_a' : 'host_b';
    // Always use single character mode (no multi model)
    useMultiModel = false;
  } else {
    // Fallback to speaker-based detection
    if (speaker === hostAName) {
      videoType = 'host_a';
      effectiveVideoMode = 'hostA';
      useMultiModel = false;
    } else if (speaker === hostBName) {
      videoType = 'host_b';
      effectiveVideoMode = 'hostB';
      useMultiModel = false;
    } else if (speaker === 'Both' || speaker.includes('Both')) {
      // Convert legacy "Both" speaker to hostA
      videoType = 'host_a';
      effectiveVideoMode = 'hostA';
      useMultiModel = false;
    }
  }
  
  // Override with explicit model from scene metadata if provided
  if (sceneMetadata?.model) {
    useMultiModel = sceneMetadata.model === 'infinite_talk_multi';
  }

  // Check cache first - look for completed videos
  const cachedVideo = await findCachedVideoByDialogue(channelId, videoType, dialogueText, '16:9');
  if (cachedVideo && cachedVideo.video_url) {
    console.log(`‚úÖ [InfiniteTalk] Using cached video for segment ${segmentIndex}`);
    return { url: cachedVideo.video_url, fromCache: true };
  }

  // Check for pending tasks - resume polling if task was started but not completed
  const pendingTask = await findPendingVideoTask(channelId, dialogueText, segmentIndex);
  if (pendingTask) {
    console.log(`üîÑ [InfiniteTalk] Resuming pending task: ${pendingTask.taskId}`);
    try {
      const videoUrl = await pollInfiniteTalkTask(pendingTask.taskId);
      if (videoUrl) {
        // Update task to completed with duration from audio
        await updateVideoTaskCompleted(pendingTask.taskId, videoUrl, audioDuration);
        
        // Track cost (estimate based on typical segment)
        const baseCost = resolution === '720p' ? INFINITETALK_COST_720P : INFINITETALK_COST_480P;
        CostTracker.track('video', 'infinitetalk-resumed', baseCost);
        
        console.log(`‚úÖ [InfiniteTalk] Resumed task completed for segment ${segmentIndex} (duration: ${audioDuration}s)`);
        return { url: videoUrl, fromCache: false, durationSeconds: audioDuration };
      }
    } catch (resumeError) {
      console.warn(`‚ö†Ô∏è [InfiniteTalk] Failed to resume task ${pendingTask.taskId}:`, (resumeError as Error).message);
      await updateVideoTaskFailed(pendingTask.taskId, (resumeError as Error).message);
      // Fall through to create new task
    }
  }

  // Get shot type from scene metadata or default to medium
  const shotType = sceneMetadata?.shot || 'medium';
  const shotDescription = shotType === 'closeup' ? 'Close-up shot, tight framing' 
    : shotType === 'wide' ? 'Wide shot, showing full studio' 
    : 'Medium shot, standard framing';

  // Declare outside try block so they're accessible in catch
  let taskId: string | undefined;
  let modelName: string = useMultiModel ? 'InfiniteTalk Multi' : 'InfiniteTalk Single';
  let savedTaskId: string | null = null; // Track if we saved the pending task

  try {
    if (useMultiModel) {
      // ===== INFINITETALK MULTI (Two characters in frame) =====
      modelName = 'InfiniteTalk Multi';
  const silentAudioUrl = getSilentAudioUrl();
  
      // For "both" scenes, use the SEPARATE audios if provided
      // Host A = left, Host B = right
      let leftAudio: string;
      let rightAudio: string;
      let order: 'left_first' | 'right_first' | 'meanwhile';
      
      if (hostA_audioUrl && hostB_audioUrl) {
        // ‚úÖ NEW: Both hosts have separate audio - use them!
        leftAudio = hostA_audioUrl;
        rightAudio = hostB_audioUrl;
        order = providedOrder || 'left_first';
        console.log(`üéôÔ∏è [${modelName}] Using SEPARATE audios for both hosts!`);
      } else {
        // Fallback: Only one audio provided, determine which side based on speaker
        const isHostASpeaking = speaker === hostAName;
        const isHostBSpeaking = speaker === hostBName;

        if (isHostASpeaking) {
    leftAudio = audioUrl;
          rightAudio = silentAudioUrl;
    order = 'left_first';
        } else if (isHostBSpeaking) {
          leftAudio = silentAudioUrl;
    rightAudio = audioUrl;
    order = 'right_first';
  } else {
          // Unknown speaker - default to left
    leftAudio = audioUrl;
          rightAudio = silentAudioUrl;
          order = 'left_first';
          console.warn(`‚ö†Ô∏è [${modelName}] Segment ${segmentIndex}: Unknown speaker '${speaker}', defaulting to left`);
        }
      }

  const characterPrompt = sceneMetadata?.scenePrompt || `
STRICT CHARACTER REQUIREMENTS - DO NOT DEVIATE:
- LEFT CHARACTER: ${hostAName} - ${hostAVisualPrompt}
- RIGHT CHARACTER: ${hostBName} - ${hostBVisualPrompt}

SCENE: Professional podcast news studio with two animated characters.
SHOT: ${shotDescription}
SPEAKING: ${effectiveVideoMode === 'hostA' ? hostAName : effectiveVideoMode === 'hostB' ? hostBName : 'Both hosts alternating'} with lip-sync animation.
STYLE: Maintain exact character appearances from the reference image.

CRITICAL: These are NOT human beings. They are animated/CGI characters as described above. 
Keep character consistency with the reference image at all times.
`.trim();

      console.log(`üé¨ [${modelName}] Generating video for segment ${segmentIndex} (${speaker})`);
      console.log(`üìù [${modelName}] Left: ${hostAName}, Right: ${hostBName}`);
      console.log(`üéôÔ∏è [${modelName}] Order: ${order}, Left audio: ${leftAudio !== silentAudioUrl ? 'ACTIVE' : 'silent'}, Right audio: ${rightAudio !== silentAudioUrl ? 'ACTIVE' : 'silent'}`);
      
      taskId = await createInfiniteTalkMultiTask({
      leftAudioUrl: leftAudio,
      rightAudioUrl: rightAudio,
      imageUrl: referenceImageUrl,
      order,
      resolution,
      prompt: characterPrompt
    });
      
    } else {
      // ===== INFINITETALK SINGLE (One character in frame) =====
      modelName = 'InfiniteTalk Single';
      
      const speakingHost = effectiveVideoMode === 'hostA' ? hostAName : hostBName;
      const visualPrompt = effectiveVideoMode === 'hostA' ? hostAVisualPrompt : hostBVisualPrompt;
      
      const characterPrompt = sceneMetadata?.scenePrompt || `
CHARACTER: ${speakingHost} - ${visualPrompt}
SCENE: Professional podcast news studio.
SHOT: ${shotDescription}
ACTION: ${speakingHost} is speaking with natural lip-sync animation.
STYLE: Maintain exact character appearance from the reference image.

CRITICAL: This is NOT a human being. This is an animated/CGI character as described above.
`.trim();

      console.log(`üé¨ [${modelName}] Generating video for segment ${segmentIndex} (${speaker})`);
      console.log(`üìù [${modelName}] Character: ${speakingHost}`);
      console.log(`üñºÔ∏è [${modelName}] Image: ${referenceImageUrl.substring(0, 60)}...`);
      
      taskId = await createInfiniteTalkSingleTask(
        audioUrl,
        referenceImageUrl,
        resolution,
        characterPrompt
      );
    }

    // ‚≠ê Save task as pending BEFORE polling - allows resume if interrupted
    savedTaskId = await saveVideoTaskPending({
      channel_id: channelId,
      production_id: productionId || null,
      video_type: videoType,
      segment_index: segmentIndex,
      prompt_hash: createPromptHash(dialogueText),
      dialogue_text: dialogueText,
      provider: 'wavespeed',
      aspect_ratio: aspectRatio, // Use channel format (16:9 or 9:16)
      duration_seconds: null,
      status: 'generating',
      error_message: null,
      reference_image_hash: createPromptHash(referenceImageUrl.substring(0, 100)),
      expires_at: null,
      task_id: taskId
    });
    
    console.log(`üíæ [${modelName}] Task ${taskId} saved as pending for segment ${segmentIndex}`);

    // Now poll for completion
    const videoUrl = await pollInfiniteTalkTask(taskId);

    if (videoUrl) {
      // Track cost (Multi is slightly more expensive)
      const baseCost = resolution === '720p' ? INFINITETALK_COST_720P : INFINITETALK_COST_480P;
      const cost = useMultiModel ? baseCost * 1.2 : baseCost; // Multi is ~20% more
      CostTracker.track('video', useMultiModel ? 'infinitetalk-multi' : 'infinitetalk-single', cost);

      // Update task to completed with duration from audio
      // Video duration = audio duration (InfiniteTalk syncs to audio)
      await updateVideoTaskCompleted(taskId, videoUrl, audioDuration);

      console.log(`‚úÖ [${modelName}] Video generated and cached for segment ${segmentIndex} (duration: ${audioDuration}s)`);
      return { url: videoUrl, fromCache: false, durationSeconds: audioDuration };
    } else {
      // Polling completed but no URL - mark as failed
      await updateVideoTaskFailed(taskId, 'Polling completed but no video URL returned');
    }
  } catch (error) {
    const errorMsg = (error as Error).message;
    console.error(`‚ùå [${modelName}] Failed for segment ${segmentIndex}:`, errorMsg);
    
    // Update task to failed if we saved a pending task
    // Note: taskId is the WaveSpeed task ID, savedTaskId is the DB record ID
    // updateVideoTaskFailed uses task_id (WaveSpeed ID) to find the record
    if (taskId) {
      await updateVideoTaskFailed(taskId, errorMsg);
    }
  }

  return { url: null, fromCache: false };
};

export const fetchEconomicNews = async (targetDate: Date | undefined, config: ChannelConfig): Promise<NewsItem[]> => {
  // Use SerpAPI for news fetching (no more Gemini quota issues!)
  console.log(`üì∞ [News] Fetching news using SerpAPI...`);
  
  try {
    const news = await fetchNewsWithSerpAPI(targetDate, config);
    console.log(`‚úÖ [News] Successfully fetched ${news.length} news items via SerpAPI`);
    return news;
  } catch (error) {
    console.error(`‚ùå [News] SerpAPI failed:`, (error as Error).message);
    throw error;
  }
};

/**
 * Convert ScriptWithScenes to ScriptLine[] for backwards compatibility
 * Extracts dialogue text from scenes and alternates between hostA and hostB
 */
export const convertScenesToScriptLines = (
  scriptWithScenes: ScriptWithScenes,
  config: ChannelConfig
): ScriptLine[] => {
  const lines: ScriptLine[] = [];
  const hostAName = config.characters.hostA.name;
  const hostBName = config.characters.hostB.name;
  
  Object.entries(scriptWithScenes.scenes)
    .sort(([a], [b]) => parseInt(a) - parseInt(b))
    .forEach(([_, scene]) => {
      // Parse dialogue from scene text - extract speaker labels if present
      const text = scene.text;
      
      // Check if scene text contains speaker labels like "Rusty:" or "Dani:"
      const dialoguePattern = new RegExp(`(${hostAName}|${hostBName}):\\s*([^${hostAName}${hostBName}]+)`, 'gi');
      const matches = [...text.matchAll(dialoguePattern)];
      
      if (matches.length > 0) {
        // Extract labeled dialogues
        matches.forEach(match => {
          const speaker = match[1];
          const dialogue = match[2].trim();
          if (dialogue) {
            lines.push({ speaker, text: dialogue });
          }
        });
      } else {
        // No labels found - assign based on video_mode
        const speaker = scene.video_mode === 'hostA' ? hostAName
          : scene.video_mode === 'hostB' ? hostBName
          : 'Both';
        lines.push({ speaker, text: text.trim() });
      }
    });
  
  return lines;
};

/**
 * Auto-improve script with retention optimization
 * Iteratively improves script until retention score reaches 80%+
 */
export const autoImproveScript = async (
  scriptWithScenes: ScriptWithScenes,
  news: NewsItem[],
  config: ChannelConfig,
  viralHook?: string,
  maxIterations: number = 3
): Promise<ScriptWithScenes> => {
  let currentScript = scriptWithScenes;
  let iteration = 0;
  let retentionScore = 0;
  
  console.log(`üîÑ [Script Improvement] Starting auto-improvement process...`);
  
  while (iteration < maxIterations) {
    // Analyze current script
    const analysis = await analyzeScriptRetention(currentScript);
    retentionScore = analysis.retentionScore;
    
    console.log(`üìä [Script Improvement] Iteration ${iteration + 1}: Retention score: ${retentionScore}%`);
    
    // If already good, stop
    if (retentionScore >= 80) {
      console.log(`‚úÖ [Script Improvement] Script optimized in ${iteration} iterations (score: ${retentionScore}%)`);
      break;
    }
    
    // Generate improvements
    const improvements = {
      implement: [
        `Increase retention score from ${retentionScore}% to 80%+`,
        `Reduce total duration to 45-60 seconds (currently ${analysis.estimatedDuration.toFixed(1)}s)`,
        ...analysis.suggestions.slice(0, 5), // Top 5 suggestions
        `Strengthen hook with viral elements (current strength: ${analysis.hookStrength}%)`,
        `Add more curiosity gaps between scenes`,
        `Use shorter sentences (5-10 words max)`,
        `Add specific numbers and statistics throughout`,
        `Cut unnecessary words - be ruthless with editing`
      ],
      maintain: [
        'Keep the core message and facts',
        'Maintain character personalities',
        'Keep factual accuracy',
        'Preserve the narrative structure'
      ]
    };
    
    console.log(`üîÑ [Script Improvement] Regenerating with ${improvements.implement.length} improvements...`);
    
    // Regenerate with improvements
    try {
      currentScript = await generateScriptWithGPT(news, config, viralHook, improvements);
      iteration++;
    } catch (error) {
      console.error(`‚ùå [Script Improvement] Regeneration failed:`, error);
      // Return current script if regeneration fails
      break;
    }
  }
  
  // Final validation
  const finalAnalysis = await analyzeScriptRetention(currentScript);
  const validation = validateScriptForVirality(currentScript);
  
  if (!validation.valid) {
    console.warn(`‚ö†Ô∏è [Script Improvement] Final script has issues:`, validation.issues);
  }
  
  console.log(`‚úÖ [Script Improvement] Final retention score: ${finalAnalysis.retentionScore}% (target: 80%+)`);
  
  return currentScript;
};

/**
 * Generate script with v2.0 Narrative Engine (returns full scene structure)
 * Now includes automatic improvement for viral retention
 */
export const generateScriptWithScenes = async (
  news: NewsItem[], 
  config: ChannelConfig, 
  viralHook?: string,
  improvements?: { implement: string[]; maintain: string[] }
): Promise<ScriptWithScenes> => {
  console.log(`üìù [Script v2.0] Generating script with Narrative Engine...`);
  if (improvements) {
    console.log(`üìù [Script v2.0] Regenerating with ${improvements.implement.length} improvements and ${improvements.maintain.length} strengths to maintain`);
  }
  
  try {
    // Generate initial script
    let scriptWithScenes = await generateScriptWithGPT(news, config, viralHook, improvements);
    console.log(`‚úÖ [Script v2.0] Generated ${Object.keys(scriptWithScenes.scenes).length} scenes using "${scriptWithScenes.narrative_used}" narrative`);
    
    // Analyze retention
    const analysis = await analyzeScriptRetention(scriptWithScenes);
    console.log(`üìä [Script v2.0] Initial retention score: ${analysis.retentionScore}%`);
    
    // Auto-improve if retention is below 80%
    if (analysis.retentionScore < 80 && !improvements) {
      console.log(`üîÑ [Script v2.0] Retention below 80%, starting auto-improvement...`);
      scriptWithScenes = await autoImproveScript(scriptWithScenes, news, config, viralHook, 3);
      
      const finalAnalysis = await analyzeScriptRetention(scriptWithScenes);
      console.log(`‚úÖ [Script v2.0] Final retention score: ${finalAnalysis.retentionScore}%`);
    }
    
    return scriptWithScenes;
  } catch (error) {
    console.error(`‚ùå [Script v2.0] GPT-4o failed:`, (error as Error).message);
    throw error;
  }
};

/**
 * Legacy function for backwards compatibility - returns ScriptLine[]
 * Internally uses v2.0 Narrative Engine and converts to legacy format
 */
export const generateScript = async (news: NewsItem[], config: ChannelConfig, viralHook?: string): Promise<ScriptLine[]> => {
  // Use GPT-4o for script generation with v2.0 Narrative Engine
  console.log(`üìù [Script] Generating script using GPT-4o (v2.0)...`);
  
  try {
    const scriptWithScenes = await generateScriptWithGPT(news, config, viralHook);
    const scriptLines = convertScenesToScriptLines(scriptWithScenes, config);
    console.log(`‚úÖ [Script] Successfully generated ${scriptLines.length} script lines via GPT-4o`);
    return scriptLines;
  } catch (error) {
    console.error(`‚ùå [Script] GPT-4o failed:`, (error as Error).message);
    throw error;
  }
};

export const fetchTrendingTopics = async (country: string): Promise<string[]> => {
  // Use SerpAPI for trending topics (no more Gemini quota issues!)
  console.log(`üìà [Trending] Fetching trending topics using SerpAPI...`);
  
  try {
    const topics = await fetchTrendingWithSerpAPI(country);
    console.log(`‚úÖ [Trending] Successfully fetched ${topics.length} trending topics via SerpAPI`);
    return topics;
  } catch (error) {
    console.error(`‚ùå [Trending] SerpAPI failed:`, (error as Error).message);
    return []; // Return empty array on failure (non-critical)
  }
};

export const generateViralMetadata = async (news: NewsItem[], config: ChannelConfig, date: Date): Promise<ViralMetadata> => {
  // Use GPT-4o for metadata generation (no more Gemini quota issues!)
  console.log(`üè∑Ô∏è [Metadata] Generating viral metadata using GPT-4o...`);
  
  // Get trending topics for SEO boost
  const trending = await fetchTrendingTopics(config.country);
  
  try {
    const metadata = await generateViralMetadataWithGPT(news, config, date, trending);
    console.log(`‚úÖ [Metadata] Successfully generated metadata via GPT-4o`);
    return metadata;
  } catch (error) {
    console.error(`‚ùå [Metadata] GPT-4o failed:`, (error as Error).message);
    // Return defaults on failure
    const fallbackTitle = "Breaking News";
    return { 
      title: fallbackTitle, 
      titleVariants: [fallbackTitle, createTitleVariantFallback(fallbackTitle)], 
      description: "", 
      tags: [] 
    };
  }
};

// Helper function to import findCachedAudio dynamically to avoid circular dependency
// Now returns { audioBase64, durationSeconds } for accurate video timing
interface CachedAudioResult {
  audioBase64: string;
  durationSeconds: number | null;
}

let findCachedAudioFn: ((text: string, voiceName: string, channelId: string) => Promise<CachedAudioResult | null>) | null = null;

export const setFindCachedAudioFunction = (fn: (text: string, voiceName: string, channelId: string) => Promise<CachedAudioResult | null>) => {
  findCachedAudioFn = fn;
};

/**
 * Extended BroadcastSegment with additional fields for "both" scenes
 * This contains audio URLs for both hosts when video_mode is "both"
 */
export interface ExtendedBroadcastSegment extends BroadcastSegment {
  // For "both" scenes - separate audio for each host
  hostA_audioBase64?: string;
  hostB_audioBase64?: string;
  hostA_audioUrl?: string;
  hostB_audioUrl?: string;
  hostA_text?: string;
  hostB_text?: string;
  // Audio order
  order?: 'left_first' | 'right_first' | 'meanwhile';
  // Scene metadata
  video_mode?: VideoMode;
  model?: 'infinite_talk' | 'infinite_talk_multi';
  shot?: ShotType;
  sceneIndex?: number;
  // Cache flags
  fromCache?: boolean;
  audioUrl?: string;
}

/**
 * Generate a single audio file for a text using TTS (OpenAI or ElevenLabs)
 * Returns audio data AND duration for accurate video timing
 * @param text - The text to convert to speech
 * @param voiceName - The OpenAI voice to use (fallback)
 * @param channelId - Channel ID for caching
 * @param label - Label for logging
 * @param language - Optional language hint (e.g., "Spanish") for better pronunciation
 * @param ttsProvider - 'openai' or 'elevenlabs'
 * @param elevenLabsVoiceId - ElevenLabs voice ID (required if ttsProvider is 'elevenlabs')
 */
const generateSingleAudio = async (
  text: string,
  voiceName: string,
  channelId: string,
  label: string,
  language?: string,
  ttsProvider: 'openai' | 'elevenlabs' = 'openai',
  elevenLabsVoiceId?: string
): Promise<{ audioBase64: string; fromCache: boolean; audioUrl?: string; durationSeconds?: number }> => {
  // Validate input text before processing
  const trimmedText = text?.trim() || '';
  if (!trimmedText) {
    console.error(`‚ùå [Audio] Empty text for ${label}, skipping TTS generation`);
    throw new Error(`Empty text provided for audio generation (${label})`);
  }
  
  // CRITICAL FIX: Load fresh configuration from DB to ensure we use the correct TTS provider
  let freshConfig: ChannelConfig | null = null;
  let effectiveProvider: 'openai' | 'elevenlabs' = ttsProvider;
  let effectiveElevenLabsVoiceId: string | undefined = elevenLabsVoiceId;
  let effectiveVoiceName: string = voiceName;
  
  if (channelId) {
    try {
      const channel = await getChannelById(channelId);
      if (channel?.config) {
        freshConfig = channel.config;
        
        // Use provider from fresh config, not from parameter
        effectiveProvider = freshConfig.ttsProvider || ttsProvider || 'openai';
        
        // Determine which character is speaking based on voiceName
        const isHostA = voiceName.toLowerCase().includes(freshConfig.characters.hostA.name.toLowerCase()) ||
                        voiceName === 'echo' ||
                        voiceName === freshConfig.characters.hostA.voiceName;
        const isHostB = voiceName.toLowerCase().includes(freshConfig.characters.hostB.name.toLowerCase()) ||
                        voiceName === 'shimmer' ||
                        voiceName === freshConfig.characters.hostB.voiceName;
        
        const character = isHostA ? freshConfig.characters.hostA : 
                         isHostB ? freshConfig.characters.hostB : 
                         freshConfig.characters.hostA; // Default to hostA
        
        // Get voice configuration from character
        effectiveVoiceName = character.voiceName;
        
        // If using ElevenLabs, get voiceId from character config
        if (effectiveProvider === 'elevenlabs') {
          effectiveElevenLabsVoiceId = character.elevenLabsVoiceId || elevenLabsVoiceId;
          
          // Validate ElevenLabs configuration
          if (!effectiveElevenLabsVoiceId) {
            console.error(`‚ùå [Audio] ElevenLabs provider selected but voiceId not configured for ${character.name}`, {
              channelId,
              character: character.name,
              characterKey: isHostA ? 'hostA' : 'hostB'
            });
            throw new Error(
              `ElevenLabs voiceId not configured for ${character.name}. ` +
              `Please configure in Admin Dashboard > Channel Settings > Character Settings.`
            );
          }
          
          // Validate ElevenLabs API is configured
          const elevenLabsConfig = checkElevenLabsConfig();
          if (!elevenLabsConfig.configured) {
            console.error(`‚ùå [Audio] ElevenLabs API key not configured`);
            throw new Error('ElevenLabs API key not configured. Please set ELEVENLABS_API_KEY in environment variables.');
          }
        }
        
        console.log(`üîç [Audio] Loaded fresh config for ${label}:`, {
          provider: effectiveProvider,
          character: character.name,
          voiceName: effectiveVoiceName,
          elevenLabsVoiceId: effectiveElevenLabsVoiceId,
          wasOverridden: effectiveProvider !== ttsProvider
        });
      } else {
        console.warn(`‚ö†Ô∏è [Audio] Channel config not found for ${channelId}, using provided parameters`);
      }
    } catch (error) {
      console.error(`‚ùå [Audio] Failed to load channel config for ${channelId}:`, error);
      // Continue with provided parameters as fallback
    }
  }
  
  // CRITICAL FIX: Determine cache key voice - INCLUDE PROVIDER to avoid returning OpenAI audio when ElevenLabs is requested
  // Format: "provider:voiceId" (e.g., "elevenlabs:9oPKasc15pfAbMr7N6Gs" or "openai:echo")
  // This ensures cache separation between providers
  const voiceId = effectiveProvider === 'elevenlabs' && effectiveElevenLabsVoiceId 
    ? effectiveElevenLabsVoiceId 
    : effectiveVoiceName;
  const cacheVoiceKey = `${effectiveProvider}:${voiceId}`;
  
  // Log provider decision for debugging
  console.log(`üéôÔ∏è [Audio] Provider decision for ${label}:`, {
    requestedProvider: ttsProvider,
    configProvider: freshConfig?.ttsProvider,
    effectiveProvider,
    voiceName: effectiveVoiceName,
    voiceId: effectiveElevenLabsVoiceId || effectiveVoiceName,
    cacheKey: cacheVoiceKey
  });
  
  // Check cache first
  if (findCachedAudioFn && channelId) {
    const cachedResult = await findCachedAudioFn(trimmedText, cacheVoiceKey, channelId);
    if (cachedResult) {
      console.log(`‚úÖ Cache hit for audio (${label}): "${trimmedText.substring(0, 30)}..." (duration: ${cachedResult.durationSeconds}s)`);
      return { 
        audioBase64: cachedResult.audioBase64, 
        fromCache: true, 
        audioUrl: cachedResult.audioBase64,
        durationSeconds: cachedResult.durationSeconds || undefined
      };
    }
  }

  let audioBase64: string;
  let audioDuration: number;
  
  // Validate ElevenLabs voiceId if provider is elevenlabs
  // ElevenLabs voice IDs are typically 20-24 character alphanumeric strings
  const isValidElevenLabsVoiceId = effectiveElevenLabsVoiceId && 
    typeof effectiveElevenLabsVoiceId === 'string' && 
    effectiveElevenLabsVoiceId.trim().length >= 15 &&
    /^[a-zA-Z0-9]+$/.test(effectiveElevenLabsVoiceId.trim());
  
  // Generate audio based on TTS provider (using effective values from fresh config)
  if (effectiveProvider === 'elevenlabs' && isValidElevenLabsVoiceId) {
    // Use ElevenLabs TTS
    console.log(`üéôÔ∏è [ElevenLabs] Generating audio for ${label} with voice: ${effectiveElevenLabsVoiceId}`);
    try {
      const result = await generateElevenLabsTTS(trimmedText, effectiveElevenLabsVoiceId!.trim());
      audioBase64 = result.audioBase64;
      audioDuration = result.audioDuration;
      console.log(`‚úÖ [ElevenLabs] Generated (${label}): "${trimmedText.substring(0, 30)}..." (${audioDuration.toFixed(1)}s)`);
    } catch (error) {
      console.error(`‚ùå [ElevenLabs] Failed for ${label}:`, (error as Error).message);
      // DO NOT fallback silently - throw error to make the issue visible
      throw new Error(
        `ElevenLabs TTS generation failed for ${label}: ${(error as Error).message}. ` +
        `Please check ElevenLabs API configuration and voiceId.`
      );
    }
  } else {
    // Use OpenAI TTS
    if (effectiveProvider === 'elevenlabs' && !isValidElevenLabsVoiceId) {
      console.error(`‚ùå [Audio] ElevenLabs selected but voiceId "${effectiveElevenLabsVoiceId}" is invalid for ${label}`);
      throw new Error(
        `ElevenLabs provider selected but voiceId is invalid or missing for ${label}. ` +
        `Please configure ElevenLabs voiceId in Admin Dashboard > Channel Settings.`
      );
    }
    audioBase64 = await generateTTSAudio(trimmedText, effectiveVoiceName, language);
    // Estimate duration from text (150 words/min = 2.5 words/sec)
    const wordCount = trimmedText.split(/\s+/).length;
    audioDuration = Math.max(1, wordCount / 2.5);
    console.log(`‚úÖ [OpenAI TTS] Generated (${label}): "${trimmedText.substring(0, 30)}..." (estimated: ${audioDuration.toFixed(1)}s)`);
  }
  
  return { audioBase64, fromCache: false, durationSeconds: audioDuration };
};

export const generateSegmentedAudio = async (script: ScriptLine[], config: ChannelConfig): Promise<BroadcastSegment[]> => {
  return generateSegmentedAudioWithCache(script, config, '');
};

/**
 * Generate audio segments from script lines (legacy format)
 * For new v2.0 format with scenes, use generateAudioFromScenes instead
 * Supports both OpenAI TTS and ElevenLabs based on config.ttsProvider
 */
export const generateSegmentedAudioWithCache = async (
  script: ScriptLine[], 
  config: ChannelConfig,
  channelId: string = ''
): Promise<BroadcastSegment[]> => {
  const ttsProvider = config.ttsProvider || 'openai';
  const providerLabel = ttsProvider === 'elevenlabs' ? 'ElevenLabs' : 'OpenAI TTS';
  
  // Log config details
  console.log(`üéôÔ∏è [Audio] Generating ${script.length} audio segments using ${providerLabel}...`);
  if (ttsProvider === 'elevenlabs') {
    console.log(`üéôÔ∏è [Audio] ElevenLabs voices: hostA=${config.characters.hostA.elevenLabsVoiceId || 'not set'}, hostB=${config.characters.hostB.elevenLabsVoiceId || 'not set'}`);
  } else {
    console.log(`üéôÔ∏è [Audio] OpenAI voices: hostA=${config.characters.hostA.voiceName}, hostB=${config.characters.hostB.voiceName}`);
  }

  // CRITICAL FIX: Improved parallel generation with batch processing
  // Process in batches of 3 to avoid overwhelming APIs while maintaining parallelism
  const BATCH_SIZE = 3;
  const batches: ScriptLine[][] = [];
  for (let i = 0; i < script.length; i += BATCH_SIZE) {
    batches.push(script.slice(i, i + BATCH_SIZE));
  }
  
  console.log(`üéôÔ∏è [Audio] Processing ${script.length} segments in ${batches.length} batches of ${BATCH_SIZE}`);
  
  const results: any[] = [];
  
  // Process batches in parallel, but limit concurrency
  for (const batch of batches) {
    const batchPromises = batch.map(async (line) => {
      // Normalize speaker name for comparison (case-insensitive, trimmed)
      const speakerNormalized = line.speaker.toLowerCase().trim();
      const hostAName = config.characters.hostA.name.toLowerCase().trim();
      const hostBName = config.characters.hostB.name.toLowerCase().trim();
      
      // Determine which character is speaking
      let character = config.characters.hostA; // default
      let characterKey = 'hostA';
      
      if (speakerNormalized === hostBName || speakerNormalized.includes(hostBName)) {
        character = config.characters.hostB;
        characterKey = 'hostB';
      } else if (speakerNormalized === hostAName || speakerNormalized.includes(hostAName)) {
        character = config.characters.hostA;
        characterKey = 'hostA';
      }
      
      // Debug log to help troubleshoot speaker matching
      console.log(`üé§ [Audio] Speaker "${line.speaker}" matched to ${characterKey} (${character.name}), voiceId: ${character.elevenLabsVoiceId || 'not set'}`);

      try {
        const result = await generateSingleAudio(
          line.text, 
          character.voiceName, 
          channelId, 
          line.speaker, 
          config.language,
          ttsProvider,
          character.elevenLabsVoiceId
        );
        return {
          speaker: line.speaker,
          text: line.text,
          audioBase64: result.audioBase64,
          fromCache: result.fromCache,
          audioUrl: result.audioUrl,
          audioDuration: result.durationSeconds // Include duration for video timing
        } as any;
      } catch (error) {
        console.error(`‚ùå [Audio] Failed for "${line.text.substring(0, 30)}...":`, (error as Error).message);
        throw error;
      }
    });
    
    // Wait for batch to complete before starting next batch
    const batchResults = await Promise.all(batchPromises);
    results.push(...batchResults);
  }
  console.log(`‚úÖ [Audio] Successfully generated ${results.length} audio segments via ${providerLabel}`);
  return results;
};

/**
 * Generate audio from v2.0 ScriptWithScenes format
 * For "both" scenes, generates SEPARATE audio for each host
 * Returns ExtendedBroadcastSegment[] with all audio data
 */
export const generateAudioFromScenes = async (
  scriptWithScenes: ScriptWithScenes,
  config: ChannelConfig,
  channelId: string = ''
): Promise<ExtendedBroadcastSegment[]> => {
  const ttsProvider = config.ttsProvider || 'openai';
  const providerLabel = ttsProvider === 'elevenlabs' ? 'ElevenLabs' : 'OpenAI TTS';
  
  console.log(`üéôÔ∏è [Audio v2.0] Generating audio from ${Object.keys(scriptWithScenes.scenes).length} scenes using ${providerLabel}...`);
  console.log(`üéôÔ∏è [Audio v2.0] Narrative: ${scriptWithScenes.narrative_used}`);
  
  if (ttsProvider === 'elevenlabs') {
    console.log(`üéôÔ∏è [Audio v2.0] ElevenLabs voices: hostA=${config.characters.hostA.elevenLabsVoiceId || 'not set'}, hostB=${config.characters.hostB.elevenLabsVoiceId || 'not set'}`);
  }
  
  const hostA = config.characters.hostA;
  const hostB = config.characters.hostB;
  
  const segments: ExtendedBroadcastSegment[] = [];
  const sceneEntries = Object.entries(scriptWithScenes.scenes).sort(([a], [b]) => parseInt(a) - parseInt(b));
  
  for (const [sceneNum, scene] of sceneEntries) {
    const sceneIndex = parseInt(sceneNum) - 1;
    
    // Legacy "both" scenes are converted to hostA (backwards compatibility)
    let effectiveVideoMode: 'hostA' | 'hostB' = scene.video_mode as 'hostA' | 'hostB';
    if ((scene.video_mode as string) === 'both') {
      console.warn(`‚ö†Ô∏è [Audio v2.0] Scene ${sceneNum}: Converting legacy "both" mode to hostA`);
      effectiveVideoMode = 'hostA';
    }
    
    {
      // SINGLE HOST SCENE: Generate 1 audio
      // Use effectiveVideoMode for proper host selection (handles legacy "both" conversion)
      const speaker = effectiveVideoMode === 'hostA' ? hostA.name : hostB.name;
      const character = effectiveVideoMode === 'hostA' ? hostA : hostB;
      // For legacy "both" scenes, try hostA_text first, then fallback to text
      const sceneText = ((scene.video_mode as string) === 'both' && scene.hostA_text) 
        ? scene.hostA_text.trim() 
        : (scene.text || '').trim();
      
      // Validate text exists
      if (!sceneText) {
        console.error(`‚ùå [Audio v2.0] Scene ${sceneNum}: No text for ${speaker}`);
        throw new Error(`Scene ${sceneNum}: Text is empty for ${speaker}`);
      }
      
      console.log(`üé¨ [Audio v2.0] Scene ${sceneNum}: ${speaker} solo - "${sceneText.substring(0, 40)}..."`);
      
      const audio = await generateSingleAudio(
        sceneText, 
        character.voiceName, 
        channelId, 
        `Scene ${sceneNum} - ${speaker}`, 
        config.language,
        ttsProvider,
        character.elevenLabsVoiceId
      );
      
      segments.push({
        speaker,
        text: sceneText,
        audioBase64: audio.audioBase64,
        video_mode: effectiveVideoMode, // Use effective mode (never "both")
        model: 'infinite_talk', // Always single model
        shot: scene.shot,
        sceneIndex,
        sceneTitle: scene.title,
        fromCache: audio.fromCache,
        audioDuration: audio.durationSeconds
      });
      
      console.log(`‚úÖ [Audio v2.0] Scene ${sceneNum}: "${scene.title || 'No title'}" - ${speaker} (${audio.durationSeconds?.toFixed(1) || '?'}s)`);
    }
  }
  
  console.log(`‚úÖ [Audio v2.0] Generated ${segments.length} segments with audio via ${providerLabel}`);
  return segments;
};

// =============================================================================================
// INFINITETALK VIDEO GENERATION FUNCTIONS (WaveSpeed Only)
// =============================================================================================

/**
 * For intro/outro, we use the reference image as a static frame
 * InfiniteTalk requires audio, so intro/outro will just use the reference image
 * The actual lip-sync videos are generated per segment
 */
export const generateIntroVideo = async (
  config: ChannelConfig,
  channelId: string,
  productionId?: string
): Promise<string | null> => {
  // For intro, prefer two-shot image (using format-aware helper), then fallback
  const twoShotUrl = getSeedImageUrl(config, 'twoShot');
  const hostAUrl = getSeedImageUrl(config, 'hostA');
  const hostBUrl = getSeedImageUrl(config, 'hostB');
  const introImage = twoShotUrl || config.referenceImageUrl || hostAUrl || hostBUrl;
  
  if (introImage) {
    console.log(`‚úÖ [Intro] Using ${twoShotUrl ? 'two-shot' : 'fallback'} image as intro frame (format: ${config.format})`);
    return introImage;
  }
  
  // If no reference image, return null (no intro video)
  console.log(`‚ö†Ô∏è [Intro] No reference image available for intro`);
  return null;
};

/**
 * Generate outro video - uses reference image as static frame
 */
export const generateOutroVideo = async (
  config: ChannelConfig,
  channelId: string,
  productionId?: string
): Promise<string | null> => {
  // For outro, prefer two-shot image (using format-aware helper), then fallback
  const twoShotUrl = getSeedImageUrl(config, 'twoShot');
  const hostAUrl = getSeedImageUrl(config, 'hostA');
  const hostBUrl = getSeedImageUrl(config, 'hostB');
  const outroImage = twoShotUrl || config.referenceImageUrl || hostAUrl || hostBUrl;
  
  if (outroImage) {
    console.log(`‚úÖ [Outro] Using ${twoShotUrl ? 'two-shot' : 'fallback'} image as outro frame (format: ${config.format})`);
    return outroImage;
  }
  
  console.log(`‚ö†Ô∏è [Outro] No reference image available for outro`);
  return null;
};

/**
 * Generate lip-sync videos for each segment using WaveSpeed InfiniteTalk Multi
 * 
 * This function takes segments WITH audio already generated and creates
 * lip-sync videos for each segment using InfiniteTalk Multi API.
 * 
 * IMPORTANT: Segments must have audioUrl (uploaded to storage) before calling this.
 * 
 * Now integrates with Scene Builder for professional-quality visual prompts.
 * 
 * @param segments - Array of segments with audio URLs
 * @param config - Channel configuration (needs referenceImageUrl)
 * @param channelId - Channel ID for caching
 * @param productionId - Production ID for caching
 * @param scriptWithScenes - Optional v2.0 script with scene metadata
 */
export const generateVideoSegmentsWithInfiniteTalk = async (
  segments: BroadcastSegment[],
  config: ChannelConfig,
  channelId: string,
  productionId?: string,
  scriptWithScenes?: ScriptWithScenes
): Promise<(string | null)[]> => {
  // Get seed images for each host using format-aware helper
  const hostASoloUrl = getSeedImageUrl(config, 'hostA');
  const hostBSoloUrl = getSeedImageUrl(config, 'hostB');
  const twoShotUrl = getSeedImageUrl(config, 'twoShot') || config.referenceImageUrl;
  
  // Check if we have at least one image to work with
  const hasAnyImage = hostASoloUrl || hostBSoloUrl || twoShotUrl || config.referenceImageUrl;
  if (!hasAnyImage) {
    console.error(`‚ùå [InfiniteTalk] No reference images found. Please set seed images in channel settings.`);
    console.error(`‚ùå [InfiniteTalk] Need at least one of: hostASoloUrl, hostBSoloUrl, twoShotUrl, or referenceImageUrl`);
    return new Array(segments.length).fill(null);
  }

  if (!isWavespeedConfigured()) {
    const configStatus = checkWavespeedConfig();
    console.error(`‚ùå [InfiniteTalk] WaveSpeed not configured: ${configStatus.message}`);
    return new Array(segments.length).fill(null);
  }

  const channelFormat = config.format || '16:9';
  console.log(`üé¨ [InfiniteTalk Multi] Generating ${segments.length} lip-sync videos (format: ${channelFormat})`);
  console.log(`üñºÔ∏è [InfiniteTalk Multi] Host A image: ${hostASoloUrl ? hostASoloUrl.substring(0, 60) + '...' : 'Not set (using fallback)'}`);
  console.log(`üñºÔ∏è [InfiniteTalk Multi] Host B image: ${hostBSoloUrl ? hostBSoloUrl.substring(0, 60) + '...' : 'Not set (using fallback)'}`);
  console.log(`üñºÔ∏è [InfiniteTalk Multi] Two-shot image: ${twoShotUrl ? twoShotUrl.substring(0, 60) + '...' : 'Not set'}`);
  
  // Log character descriptions to verify they're being used
  const hostAName = config.characters.hostA.name;
  const hostBName = config.characters.hostB.name;
  const hostAVisualPrompt = config.characters.hostA.visualPrompt;
  const hostBVisualPrompt = config.characters.hostB.visualPrompt;
  
  console.log(`üë§ [InfiniteTalk Multi] Host A: ${hostAName} - ${hostAVisualPrompt}`);
  console.log(`üë§ [InfiniteTalk Multi] Host B: ${hostBName} - ${hostBVisualPrompt}`);
  
  // === SCENE BUILDER INTEGRATION ===
  // Generate optimized visual prompts for each scene using Scene Builder
  let scenePrompts: ScenePrompt[] = [];
  if (scriptWithScenes) {
    console.log(`üìñ [InfiniteTalk Multi] Using v2.0 Narrative Engine: ${scriptWithScenes.narrative_used}`);
    console.log(`üìñ [InfiniteTalk Multi] Scene count: ${Object.keys(scriptWithScenes.scenes).length}`);
    
    // Generate scene prompts with Scene Builder (validates shot types and adds visual details)
    scenePrompts = generateScenePrompts(scriptWithScenes, config);
    console.log(`üé® [Scene Builder] Generated ${scenePrompts.length} optimized visual prompts`);
    
    // Log shot corrections made by Scene Builder
    scenePrompts.forEach((sp, idx) => {
      const originalShot = Object.values(scriptWithScenes.scenes)[idx]?.shot;
      if (originalShot !== sp.scene.shot) {
        console.log(`üé¨ [Scene Builder] Scene ${idx + 1}: Shot corrected from ${originalShot} to ${sp.scene.shot}`);
      }
    });
  }

  // =============================================================================================
  // PARALLEL VIDEO GENERATION - Send all requests at once with staggered delays
  // =============================================================================================
  // Instead of processing in sequential batches (slow), we send ALL video requests
  // in parallel with small staggered delays to avoid rate limits. This dramatically
  // reduces total generation time from O(n * batch_time) to O(max_video_time + small_delays)
  //
  // TIMING INFO (WaveSpeed InfiniteTalk):
  // - Average generation time: ~400 seconds (~6.7 minutes)
  // - Slow cases: up to 700+ seconds (~12 minutes)
  // - By sending in parallel, total time ‚âà slowest video + stagger delays
  // - Example: 8 videos √ó 400s avg = would be 53+ min sequential, but ~7 min parallel!
  
  const STAGGER_DELAY_MS = 500; // Delay between starting each video (500ms)
  const AVG_VIDEO_TIME_MS = 400000; // ~6.7 minutes average
  const videoUrls: (string | null)[] = new Array(segments.length).fill(null);
  const failedIndices: number[] = [];
  const missingAudioIndices: number[] = [];
  
  // Build scene metadata lookup from scriptWithScenes (scene index -> metadata)
  // Now includes visual prompts from Scene Builder
  const sceneMetadataMap: Map<number, { 
    video_mode: VideoMode; 
    model: 'infinite_talk' | 'infinite_talk_multi'; 
    shot: ShotType;
    scenePrompt?: string;
    lightingMood?: string;
    expressionHint?: string;
    seedImageUrl?: string;
    cameraAngle?: 'eye_level' | 'high_angle' | 'low_angle' | 'bird_eye' | 'worm_eye';
  }> = new Map();
  
  if (scriptWithScenes?.scenes) {
    // Sort by scene number to ensure stable mapping to segment indices (0..n-1)
    Object.entries(scriptWithScenes.scenes)
      .sort(([a], [b]) => parseInt(a) - parseInt(b))
      .forEach(([sceneNum, scene], idx) => {
      const scenePrompt = scenePrompts[idx];
      sceneMetadataMap.set(idx, {
        video_mode: scene.video_mode,
        model: scene.model,
        shot: scenePrompt?.scene.shot || scene.shot, // Use corrected shot from Scene Builder
        scenePrompt: scenePrompt?.visualPrompt, // Use optimized visual prompt
        lightingMood: scenePrompt?.lightingMood,
        expressionHint: scenePrompt?.expressionHint,
        seedImageUrl: (scenePrompt as any)?.seedImageUrl,
        cameraAngle: (scenePrompt as any)?.cameraAngle
      });
    });
  }

  // === PRE-VALIDATION: Check all audio URLs exist before starting ===
  segments.forEach((segment, idx) => {
    const audioUrl = (segment as any).audioUrl;
    if (!audioUrl) {
      missingAudioIndices.push(idx);
    }
  });
  
  if (missingAudioIndices.length > 0) {
    console.warn(`‚ö†Ô∏è [InfiniteTalk] ${missingAudioIndices.length} segments missing audio URLs: [${missingAudioIndices.join(', ')}]`);
    console.warn(`‚ö†Ô∏è [InfiniteTalk] These segments will be skipped. Ensure audio is uploaded before video generation.`);
  }

  // =============================================================================================
  // IMPROVED PARALLEL PROCESSING: Batch processing for better resource management
  // =============================================================================================
  const VIDEO_BATCH_SIZE = 3; // Process 3 videos at a time to balance speed and API limits
  const totalStaggerTime = (segments.length - 1) * STAGGER_DELAY_MS;
  const estimatedTotalTime = Math.round((AVG_VIDEO_TIME_MS + totalStaggerTime) / 60000);
  
  console.log(`üöÄ [InfiniteTalk] Launching ${segments.length} video tasks in ${Math.ceil(segments.length / VIDEO_BATCH_SIZE)} batches`);
  console.log(`‚è±Ô∏è [InfiniteTalk] Estimated time: ~${estimatedTotalTime} minutes (avg ${Math.round(AVG_VIDEO_TIME_MS / 60000)} min/video + stagger)`);
  
  const startTime = Date.now();
  
  // CRITICAL FIX: Process videos in batches for better resource management
  // This prevents overwhelming the API while still maintaining parallelism
  const videoBatches: BroadcastSegment[][] = [];
  for (let i = 0; i < segments.length; i += VIDEO_BATCH_SIZE) {
    videoBatches.push(segments.slice(i, i + VIDEO_BATCH_SIZE));
  }
  
  // Create all video generation promises with staggered delays
  const videoPromises = segments.map(async (segment, globalIndex) => {
    // Stagger the start of each request to avoid overwhelming the API
    await new Promise(resolve => setTimeout(resolve, globalIndex * STAGGER_DELAY_MS));
    
    const audioUrl = (segment as any).audioUrl;
    
    if (!audioUrl) {
      // Already warned above, just skip
      return { index: globalIndex, url: null, reason: 'missing_audio' };
    }
    
    // Use originalIndex if provided (for single segment regeneration), otherwise use globalIndex
    const sceneIndex = (segment as any).originalIndex ?? globalIndex;
    
    // Get scene metadata if available (now includes Scene Builder visual prompts)
    const sceneMetadata = sceneMetadataMap.get(sceneIndex);

    // Always use single model (multi model disabled for dynamic single-character scenes)
    // Legacy "both" and "infinite_talk_multi" are converted to single host
    const useMultiModel = false;

    // Determine which image to use based on video_mode
    // Always use solo image of the speaking host for dynamic single-character scenes
    let imageUrlForSegment: string;
    
    {
      // Single model: Use solo image of the speaking host
      // IMPORTANT: Use sceneMetadata.video_mode first, then fall back to speaker name matching
      const videoMode = sceneMetadata?.video_mode || (segment.speaker === hostAName ? 'hostA' : 'hostB');
      
      console.log(`üé≠ [InfiniteTalk] Segment ${sceneIndex}: video_mode=${videoMode}, speaker=${segment.speaker}, hostA=${hostAName}, hostB=${hostBName}`);
      
      // Prefer per-scene seed image variation if explicitly provided by Scene Builder
      if (sceneMetadata?.seedImageUrl) {
        imageUrlForSegment = sceneMetadata.seedImageUrl;
        console.log(
          `üñºÔ∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using Scene Builder seed variation` +
          `${sceneMetadata.cameraAngle ? ` (cameraAngle: ${sceneMetadata.cameraAngle})` : ''}`
        );
      } else {
        // NEW: If the channel has `seed_image_variations` in config, use them to vary the framing.
        // We map shot types to the closest available camera angle when a cameraAngle isn't provided.
        const shotToAngle = (shot?: ShotType): 'eye_level' | 'low_angle' | 'high_angle' | 'closeup' | 'wide' => {
          switch (shot) {
            case 'extreme_closeup':
            case 'closeup':
              return 'closeup';
            case 'wide':
            case 'medium_wide':
              return 'wide';
            // These are "style" shots; fall back to a neutral framing unless a cameraAngle is explicitly given
            case 'dutch_angle':
            case 'over_shoulder':
            case 'medium_closeup':
            case 'medium':
            default:
              return 'eye_level';
          }
        };

        const hostType: 'hostA' | 'hostB' = videoMode === 'hostA' ? 'hostA' : 'hostB';
        const effectiveAngle = (sceneMetadata?.cameraAngle as any) || shotToAngle(sceneMetadata?.shot);
        const variationUrl = getSeedImageForScene(config, hostType, effectiveAngle);

        if (variationUrl) {
          imageUrlForSegment = variationUrl;
          console.log(
            `üñºÔ∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using seed_image_variations (${hostType}, angle=${effectiveAngle})`
          );
        } else if (videoMode === 'hostA' && hostASoloUrl) {
          imageUrlForSegment = hostASoloUrl;
          console.log(`üñºÔ∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using Host A solo image (video_mode: hostA)`);
        } else if (videoMode === 'hostB' && hostBSoloUrl) {
          imageUrlForSegment = hostBSoloUrl;
          console.log(`üñºÔ∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using Host B solo image (video_mode: hostB)`);
        } else if (segment.speaker === hostAName && hostASoloUrl) {
          imageUrlForSegment = hostASoloUrl;
          console.log(`üñºÔ∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using Host A solo image (by speaker name: ${segment.speaker})`);
        } else if (segment.speaker === hostBName && hostBSoloUrl) {
          imageUrlForSegment = hostBSoloUrl;
          console.log(`üñºÔ∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using Host B solo image (by speaker name: ${segment.speaker})`);
        } else {
          // Fallback: use any available solo image or two-shot
          imageUrlForSegment = hostASoloUrl || hostBSoloUrl || twoShotUrl || config.referenceImageUrl || '';
          console.warn(`‚ö†Ô∏è [InfiniteTalk Single] Segment ${sceneIndex}: Using fallback image (no match for video_mode=${videoMode}, speaker=${segment.speaker})`);
        }
      }
    }

    if (!imageUrlForSegment) {
      console.error(`‚ùå [InfiniteTalk] Segment ${sceneIndex}: No image available`);
      return { index: globalIndex, url: null, reason: 'no_image' };
    }

    // Get extended segment data (for "both" scenes with separate audios)
    const extSegment = segment as ExtendedBroadcastSegment;
    const hostA_audioUrl = extSegment.hostA_audioUrl;
    const hostB_audioUrl = extSegment.hostB_audioUrl;
    const order = extSegment.order;
    // Get audio duration for accurate video timing
    const segmentDuration = segment.audioDuration;

    console.log(`üé¨ [InfiniteTalk] Starting segment ${sceneIndex + 1}/${segments.length} (scene index: ${sceneIndex})...`);

    // Use retry logic for video generation (static import to avoid Vercel code-splitting issues)
    const result = await retryVideoGeneration(
      async () => {
        const videoResult = await generateInfiniteTalkVideo({
          channelId,
          productionId,
          segmentIndex: sceneIndex,  // Use the correct scene index
          audioUrl,
          referenceImageUrl: imageUrlForSegment,
          speaker: segment.speaker,
          dialogueText: segment.text,
          hostAName,
          hostBName,
          hostAVisualPrompt,
          hostBVisualPrompt,
          resolution: '720p',
          aspectRatio: channelFormat, // Use channel format (16:9 or 9:16)
          // Pass separate audios for "both" scenes
          hostA_audioUrl,
          hostB_audioUrl,
          order,
          sceneMetadata,
          // Pass audio duration for accurate video timing
          audioDuration: segmentDuration
        });
        return videoResult.url;
      },
      {
        maxRetries: 2,
        continueOnError: true,
        onFailure: (error, attempt) => {
          console.warn(`‚ö†Ô∏è [InfiniteTalk] Segment ${globalIndex} failed (attempt ${attempt}):`, (error as Error).message);
        }
      }
    );

    if (result) {
      console.log(`‚úÖ [InfiniteTalk] Segment ${sceneIndex + 1} complete`);
    } else {
      failedIndices.push(sceneIndex);
    }

    // Return globalIndex for array placement, but use sceneIndex for scene metadata
    return { index: globalIndex, url: result, sceneIndex };
  });

  // Wait for ALL videos to complete in parallel
  console.log(`‚è≥ [InfiniteTalk] Waiting for all ${segments.length} videos to complete...`);
  const allResults = await Promise.allSettled(videoPromises);
  
  const elapsedTime = ((Date.now() - startTime) / 1000).toFixed(1);
  console.log(`‚è±Ô∏è [InfiniteTalk] All video tasks completed in ${elapsedTime}s`);

  // Process all results
  allResults.forEach((result, idx) => {
    if (result.status === 'fulfilled') {
      videoUrls[result.value.index] = result.value.url;
    } else {
      failedIndices.push(idx);
      console.error(`‚ùå [InfiniteTalk] Segment ${idx} failed:`, result.reason);
    }
  });
  
  const successCount = videoUrls.filter(url => url !== null).length;
  const skippedCount = missingAudioIndices.length;
  const failedCount = failedIndices.length - skippedCount;
  
  console.log(`‚úÖ [InfiniteTalk Multi] Generated ${successCount}/${segments.length} videos`);
  if (skippedCount > 0) {
    console.log(`‚è≠Ô∏è [InfiniteTalk Multi] Skipped ${skippedCount} (missing audio)`);
  }
  if (failedCount > 0) {
    console.log(`‚ùå [InfiniteTalk Multi] Failed ${failedCount} (generation errors)`);
  }

  return videoUrls;
};

/**
 * Legacy function for compatibility - now just returns empty arrays
 * Actual video generation happens in generateVideoSegmentsWithInfiniteTalk
 */
export const generateVideoSegments = async (
  script: ScriptLine[],
  config: ChannelConfig,
  channelId: string,
  productionId?: string
): Promise<(string | null)[]> => {
  console.log(`‚ö†Ô∏è [Video Generation] generateVideoSegments called but videos are generated separately with InfiniteTalk`);
  console.log(`‚ö†Ô∏è [Video Generation] Use generateVideoSegmentsWithInfiniteTalk after audio is uploaded`);
  return new Array(script.length).fill(null);
};

/**
 * Generate or retrieve intro/outro for a channel
 * 
 * CURRENT STATUS: Intro/outro are disabled (always null)
 * The composition will start directly with the first video segment
 * 
 * To enable intro/outro in the future:
 * 1. Generate intro/outro videos separately using InfiniteTalk or another tool
 * 2. Upload them to Supabase Storage
 * 3. Update this function to load and return the video URLs
 */
export const generateBroadcastVisuals = async (
  newsContext: string,
  config: ChannelConfig,
  script: ScriptLine[],
  channelId: string,
  productionId?: string
): Promise<VideoAssets> => {
  console.log(`[Broadcast Visuals] Skipping intro/outro - videos will start directly with scenes`);
  
  // Reference image for other purposes (thumbnail generation, etc.) - using format-aware helper
  const referenceImage = config.referenceImageUrl || getSeedImageUrl(config, 'twoShot') || null;

  return {
    intro: null,
    outro: null,
    wide: referenceImage,
    hostA: [],
    hostB: []
  };
};

// Helper function to convert image URL to data URI
const imageUrlToDataUri = async (imageUrl: string): Promise<string> => {
  try {
    const response = await fetch(imageUrl);
    const blob = await response.blob();
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => resolve(reader.result as string);
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  } catch (error) {
    console.error("Failed to convert image URL to data URI:", error);
    throw error;
  }
};

export const generateReferenceImage = async (
  config: ChannelConfig,
  sceneDescription?: string
): Promise<string | null> => {
  // Build comprehensive prompt based on channel config and optional scene description
  const defaultScene = sceneDescription || `Professional news studio setting with ${config.tone} atmosphere`;
  
  // CRITICAL: Extract character type from visual prompts to enforce in the image
  const hostADesc = config.characters.hostA.visualPrompt;
  const hostBDesc = config.characters.hostB.visualPrompt;
  
  const prompt = `
STRICT IMAGE GENERATION REQUIREMENTS - READ CAREFULLY:

=== CHARACTERS (MANDATORY - DO NOT CHANGE) ===
LEFT CHARACTER (${config.characters.hostA.name}): ${hostADesc}
RIGHT CHARACTER (${config.characters.hostB.name}): ${hostBDesc}

=== CRITICAL RESTRICTIONS ===
- DO NOT generate human beings under any circumstances
- DO NOT replace the characters with humans
- MUST create exactly the characters described above
- Both characters MUST be clearly visible and recognizable
- Position: ${config.characters.hostA.name} on the LEFT, ${config.characters.hostB.name} on the RIGHT

=== SCENE SETUP ===
- Setting: Professional podcast/news broadcast studio
- Layout: Two-person desk setup with microphones
- Lighting: Professional studio lighting
- Background: Modern news studio with screens/monitors
- Aspect Ratio: ${config.format}
- Quality: High resolution, broadcast quality

=== CHANNEL BRANDING ===
- Channel: ${config.channelName}
- Tagline: ${config.tagline}
- Tone: ${config.tone}

=== COMPOSITION ===
- Both characters seated at a professional desk
- Microphones visible in front of each character
- Studio monitors or screens in background
- Professional news studio environment
- Characters should appear ready to present news

FINAL CHECK: Verify the image contains EXACTLY the characters described (${hostADesc} and ${hostBDesc}), NOT humans.
`.trim();

  try {
    // Use WaveSpeed Nano Banana Pro Edit as primary
    if (isWavespeedConfigured()) {
      console.log(`üñºÔ∏è [Image] Generating reference image using WaveSpeed Nano Banana Pro...`);
      
      const aspectRatio = config.format === '9:16' ? '9:16' : '16:9';
      const placeholderImage = createPlaceholderImage(
        aspectRatio === '9:16' ? 576 : 1024,
        aspectRatio === '9:16' ? 1024 : 576
      );
      const taskId = await createWavespeedImageTask(prompt, aspectRatio, placeholderImage);
      const imageUrl = await pollWavespeedImageTask(taskId);
      
      const dataUri = await imageUrlToDataUri(imageUrl);
      CostTracker.track('thumbnail', 'wavespeed/nano-banana-pro', 0.14);
      
      console.log(`‚úÖ [Image] Reference image generated via WaveSpeed`);
      return dataUri;
    }
  } catch (wavespeedError) {
    console.error(`‚ö†Ô∏è [Image] WaveSpeed failed, trying DALL-E fallback:`, (wavespeedError as Error).message);
  }

  // Fallback to DALL-E 3
  try {
    console.log(`üñºÔ∏è [Image] Generating reference image using DALL-E 3 (fallback)...`);
    
    const size = config.format === '9:16' ? '1024x1792' : '1792x1024';
    const dalleImage = await generateImageWithDALLE(prompt, size as any);
    
    if (dalleImage) {
      console.log(`‚úÖ [Image] Reference image generated via DALL-E 3`);
      return dalleImage;
    }
  } catch (dalleError) {
    console.error(`‚ùå [Image] DALL-E 3 also failed:`, (dalleError as Error).message);
  }

  return null;
};

export const generateThumbnail = async (newsContext: string, config: ChannelConfig): Promise<string | null> => {
  // Enhanced prompt for better thumbnail quality
  const prompt = `
  Create a high-impact YouTube thumbnail for a news video about: ${newsContext}.
  Channel Style: ${config.channelName} (${config.tone}).
  
  VISUAL REQUIREMENTS:
  - Bold, high contrast, breaking news style
  - Vibrant colors that pop on mobile screens
  - Professional news aesthetic with modern design
  - Clear focal point that draws the eye
  - Include text overlay if possible or just striking imagery
  - Aspect Ratio: 16:9 (1280x720)
  - No photorealistic faces of real politicians if restricted, use stylized or symbolic representations
  - Use icons, symbols, or abstract representations when needed
  - Ensure text is large and readable at thumbnail size
`.trim();

  // Try WaveSpeed generation model first (no placeholder needed)
  if (isWavespeedConfigured()) {
    try {
      console.log(`üé® [Thumbnail] Generating thumbnail using WaveSpeed (generation model)...`);
      
      // Try generation model first (no input image)
      const taskId = await createWavespeedImageTask(prompt, '16:9');
      const imageUrl = await pollWavespeedImageTask(taskId);
      
      const dataUri = await imageUrlToDataUri(imageUrl);
      CostTracker.track('thumbnail', 'wavespeed/nano-banana-pro', 0.14);
      
      console.log(`‚úÖ [Thumbnail] Generated via WaveSpeed`);
      return dataUri;
    } catch (wavespeedError) {
      console.error(`‚ö†Ô∏è [Thumbnail] WaveSpeed generation failed, trying edit model:`, (wavespeedError as Error).message);
      
      // Fallback to edit model with placeholder
      try {
        console.log(`üé® [Thumbnail] Trying WaveSpeed edit model with placeholder...`);
        const placeholderImage = createPlaceholderImage(1024, 576);
        const taskId = await createWavespeedImageTask(prompt, '16:9', placeholderImage);
        const imageUrl = await pollWavespeedImageTask(taskId);
        
        const dataUri = await imageUrlToDataUri(imageUrl);
        CostTracker.track('thumbnail', 'wavespeed/nano-banana-pro', 0.14);
        
        console.log(`‚úÖ [Thumbnail] Generated via WaveSpeed (edit model)`);
        return dataUri;
      } catch (editError) {
        console.error(`‚ö†Ô∏è [Thumbnail] WaveSpeed edit model also failed, trying DALL-E:`, (editError as Error).message);
      }
    }
  }

  // Enhanced DALL-E prompt for better quality
  const enhancedDallePrompt = `
  Create a professional, high-impact YouTube thumbnail image for a news video.
  
  TOPIC: ${newsContext}
  CHANNEL: ${config.channelName} (${config.tone} tone)
  
  DESIGN SPECIFICATIONS:
  - Style: Modern news broadcast aesthetic, similar to CNN, BBC, or Fox News thumbnails
  - Composition: Bold, eye-catching layout with strong visual hierarchy
  - Colors: High contrast, vibrant palette that works on mobile (avoid muted tones)
  - Typography: Large, bold text overlay if applicable, highly readable at small sizes
  - Visual elements: Use icons, symbols, charts, or abstract graphics rather than photorealistic people when possible
  - Lighting: Dramatic, professional studio lighting with clear shadows and highlights
  - Mood: Urgent, breaking news atmosphere
  
  TECHNICAL:
  - Aspect ratio: 16:9 (1280x720 equivalent)
  - Resolution: High quality, sharp details
  - No watermarks, no text that might be cut off
  - Professional news channel aesthetic
  
  Make it CLICK-WORTHY and visually striking!
`.trim();

  // Fallback to DALL-E 3 with enhanced prompt
  try {
    console.log(`üé® [Thumbnail] Generating thumbnail using DALL-E 3 (enhanced prompt)...`);
    const dalleImage = await generateImageWithDALLE(enhancedDallePrompt, '1792x1024');
    
    if (dalleImage) {
      console.log(`‚úÖ [Thumbnail] Generated via DALL-E 3`);
      return dalleImage;
    }
  } catch (dalleError) {
    console.error(`‚ùå [Thumbnail] DALL-E 3 failed:`, (dalleError as Error).message);
  }

  return null;
};

// Generate seed image for Narrative Engine hosts
export const generateSeedImage = async (prompt: string, aspectRatio: '1:1' | '16:9' | '9:16' = '1:1'): Promise<string | null> => {
  // Try WaveSpeed first
  if (isWavespeedConfigured()) {
    try {
      console.log(`üé® [SeedImage] Generating seed image using WaveSpeed (${aspectRatio})...`);
      const taskId = await createWavespeedImageTask(prompt, aspectRatio);
      const imageUrl = await pollWavespeedImageTask(taskId);
      const dataUri = await imageUrlToDataUri(imageUrl);
      CostTracker.track('seed_image', 'wavespeed/nano-banana-pro', 0.14);
      console.log(`‚úÖ [SeedImage] Generated via WaveSpeed`);
      return dataUri;
    } catch (wavespeedError) {
      console.error(`‚ö†Ô∏è [SeedImage] WaveSpeed failed:`, (wavespeedError as Error).message);
    }
  }

  // Fallback to DALL-E
  try {
    console.log(`üé® [SeedImage] Generating seed image using DALL-E 3 (${aspectRatio})...`);
    // DALL-E 3 sizes: 1024x1024 (1:1), 1792x1024 (16:9), 1024x1792 (9:16)
    const size = aspectRatio === '16:9' ? '1792x1024' : aspectRatio === '9:16' ? '1024x1792' : '1024x1024';
    const dalleImage = await generateImageWithDALLE(prompt, size as '1024x1024' | '1792x1024' | '1024x1792');
    if (dalleImage) {
      console.log(`‚úÖ [SeedImage] Generated via DALL-E 3`);
      return dalleImage;
    }
  } catch (dalleError) {
    console.error(`‚ùå [SeedImage] DALL-E 3 failed:`, (dalleError as Error).message);
  }

  return null;
};

/**
 * Thumbnail Analysis Interface
 */
export interface ThumbnailAnalysis {
  thumbnailUrl: string;
  style: string;
  predictedCTR: number; // 0-100
  elements: {
    hasFace: boolean;
    hasText: boolean;
    hasNumber: boolean;
    hasEmoji: boolean;
    colorContrast: 'high' | 'medium' | 'low';
    textReadability: 'high' | 'medium' | 'low';
  };
  strengths: string[];
  weaknesses: string[];
}

/**
 * Generate multiple thumbnail variants with A/B testing analysis
 * NEW: Generates 5-10 variants and analyzes each for optimal CTR
 * 
 * Returns both new format (with analysis) and legacy format (primary/variant) for compatibility
 */
export const generateThumbnailVariantsAdvanced = async (
  newsContext: string,
  config: ChannelConfig,
  viralMeta: ViralMetadata,
  channelId?: string,
  productionId?: string,
  variantCount: number = 8 // Generate 8 variants by default
): Promise<{ 
  variants: Array<{ url: string; analysis: ThumbnailAnalysis }>;
  best: { url: string; analysis: ThumbnailAnalysis } | null;
}> => {
  /**
   * Analyze thumbnail for CTR prediction
   * MOVED UP: Must be defined before use
   */
  const analyzeThumbnail = (url: string | null, styleName: string): ThumbnailAnalysis => {
    if (!url) {
      return {
        thumbnailUrl: '',
        style: styleName,
        predictedCTR: 0,
        elements: {
          hasFace: false,
          hasText: false,
          hasNumber: false,
          hasEmoji: false,
          colorContrast: 'low',
          textReadability: 'low'
        },
        strengths: [],
        weaknesses: ['No thumbnail generated']
      };
    }
    
    // Basic analysis based on style (can be enhanced with image analysis API)
    const styleAnalysis: Record<string, Partial<ThumbnailAnalysis>> = {
      'Shocked Face + Bold Text': {
        elements: { hasFace: true, hasText: true, hasNumber: false, hasEmoji: false, colorContrast: 'high', textReadability: 'high' },
        predictedCTR: 75,
        strengths: ['Emotional impact', 'High contrast', 'Readable text'],
        weaknesses: []
      },
      'Split Screen Comparison': {
        elements: { hasFace: false, hasText: true, hasNumber: true, hasEmoji: false, colorContrast: 'high', textReadability: 'high' },
        predictedCTR: 70,
        strengths: ['Visual comparison', 'Numbers/statistics', 'Clear contrast'],
        weaknesses: []
      },
      'Symbolic + Urgency': {
        elements: { hasFace: false, hasText: true, hasNumber: false, hasEmoji: true, colorContrast: 'high', textReadability: 'medium' },
        predictedCTR: 68,
        strengths: ['Urgency', 'Symbolic meaning', 'Color impact'],
        weaknesses: []
      },
      'Number Focus': {
        elements: { hasFace: false, hasText: true, hasNumber: true, hasEmoji: false, colorContrast: 'high', textReadability: 'high' },
        predictedCTR: 72,
        strengths: ['Clear statistic', 'High readability', 'Professional'],
        weaknesses: []
      },
      'Question Hook': {
        elements: { hasFace: false, hasText: true, hasNumber: false, hasEmoji: false, colorContrast: 'medium', textReadability: 'high' },
        predictedCTR: 65,
        strengths: ['Creates curiosity', 'Engaging question'],
        weaknesses: ['May need stronger visual']
      },
      'Breaking News Banner': {
        elements: { hasFace: false, hasText: true, hasNumber: false, hasEmoji: false, colorContrast: 'high', textReadability: 'high' },
        predictedCTR: 73,
        strengths: ['Urgency', 'Professional style', 'High contrast'],
        weaknesses: []
      },
      'Before/After Timeline': {
        elements: { hasFace: false, hasText: true, hasNumber: true, hasEmoji: false, colorContrast: 'high', textReadability: 'medium' },
        predictedCTR: 67,
        strengths: ['Visual progression', 'Data visualization'],
        weaknesses: ['May be complex for small size']
      },
      'Emotional Close-up': {
        elements: { hasFace: true, hasText: true, hasNumber: false, hasEmoji: false, colorContrast: 'high', textReadability: 'high' },
        predictedCTR: 76,
        strengths: ['Strong emotional impact', 'Human connection', 'High contrast'],
        weaknesses: []
      },
      'Data Visualization': {
        elements: { hasFace: false, hasText: true, hasNumber: true, hasEmoji: false, colorContrast: 'medium', textReadability: 'medium' },
        predictedCTR: 64,
        strengths: ['Data-driven', 'Professional'],
        weaknesses: ['May be less emotional']
      },
      'Contrast Split': {
        elements: { hasFace: false, hasText: true, hasNumber: false, hasEmoji: false, colorContrast: 'high', textReadability: 'high' },
        predictedCTR: 69,
        strengths: ['Visual metaphor', 'High contrast', 'Clear concept'],
        weaknesses: []
      }
    };
    
    const analysis = styleAnalysis[styleName] || {
      elements: { hasFace: false, hasText: true, hasNumber: false, hasEmoji: false, colorContrast: 'medium', textReadability: 'medium' },
      predictedCTR: 60,
      strengths: [],
      weaknesses: []
    };
    
    return {
      thumbnailUrl: url,
      style: styleName,
      predictedCTR: analysis.predictedCTR || 60,
      elements: analysis.elements || {
        hasFace: false,
        hasText: true,
        hasNumber: false,
        hasEmoji: false,
        colorContrast: 'medium',
        textReadability: 'medium'
      },
      strengths: analysis.strengths || [],
      weaknesses: analysis.weaknesses || []
    };
  };

  // ‚≠ê Check cache first - avoid regenerating for same context
  if (channelId) {
    const cached = await findCachedThumbnail(channelId, newsContext, viralMeta.title);
    if (cached) {
      console.log(`‚úÖ [Thumbnails] Using cached thumbnails (used ${cached.useCount} times before)`);
      // Return cached thumbnails with basic analysis
      const cachedVariants = [
        { url: cached.thumbnailUrl, analysis: analyzeThumbnail(cached.thumbnailUrl, 'cached') }
      ];
      if (cached.variantUrl) {
        cachedVariants.push({ url: cached.variantUrl, analysis: analyzeThumbnail(cached.variantUrl, 'cached') });
      }
      return {
        variants: cachedVariants,
        best: cachedVariants[0] || null
      };
    }
  }

  // CRITICAL FIX: Expanded thumbnail styles for A/B testing (8+ styles)
  const styles = [
    {
      name: "Shocked Face + Bold Text",
      prompt: "Close-up shocked/surprised expression with mouth open, bold oversized text overlay with the headline, high contrast bright colors, dramatic lighting from one side"
    },
    {
      name: "Split Screen Comparison",
      prompt: "Split-screen vertical composition showing before/after or two contrasting elements, bold arrows pointing between them, numbers/percentages overlaid, contrasting color schemes on each side"
    },
    {
      name: "Symbolic + Urgency",
      prompt: "Symbolic representation of the topic (money, charts, danger symbols), urgent red/yellow color scheme, text with exclamation marks, clock or fire emoji elements"
    },
    {
      name: "Number Focus",
      prompt: "Large prominent number or statistic in center, bold text overlay, minimal background, high contrast, professional news aesthetic"
    },
    {
      name: "Question Hook",
      prompt: "Intriguing visual question mark or question text, mysterious background, bold text asking a compelling question, creates curiosity gap"
    },
    {
      name: "Breaking News Banner",
      prompt: "Red breaking news banner at top, dramatic scene below, bold white text overlay, urgent color scheme, professional broadcast style"
    },
    {
      name: "Before/After Timeline",
      prompt: "Timeline visualization showing progression, bold arrows, contrasting colors for different time periods, numbers and percentages highlighted"
    },
    {
      name: "Emotional Close-up",
      prompt: "Extreme close-up of expressive face showing strong emotion (shock, concern, excitement), bold text overlay, shallow depth of field, high emotional impact"
    },
    {
      name: "Data Visualization",
      prompt: "Charts, graphs, or data visualizations prominently displayed, bold text overlay with key statistic, professional infographic style, high contrast"
    },
    {
      name: "Contrast Split",
      prompt: "Vertical or horizontal split showing two contrasting concepts, bold text on each side, dramatic color contrast, visual metaphor for the story"
    }
  ];

  // Rotate style based on date (simple A/B pattern)
  const styleIndex = new Date().getDate() % styles.length;
  const primaryStyle = styles[styleIndex];
  const variantStyle = styles[(styleIndex + 1) % styles.length];

  const basePrompt = (style: typeof styles[0]) => `
Create a VIRAL, high-performance YouTube thumbnail image.

HEADLINE: "${viralMeta.title}"
TOPIC: ${newsContext}
CHANNEL: ${config.channelName} (${config.tone} tone)

THUMBNAIL STYLE: ${style.name}
${style.prompt}

DESIGN REQUIREMENTS:
- Aspect ratio: 16:9 (1280x720 equivalent)
- Text overlay: Large, bold, highly readable text displaying "${viralMeta.title.substring(0, 40)}"
- Color scheme: Use brand colors ${config.logoColor1} and ${config.logoColor2} prominently, with high contrast
- Mobile optimization: Must be clear and impactful when viewed at small sizes (thumbnails are tiny!)
- Emotional impact: Evoke shock, curiosity, or urgency through visual composition
- Professional quality: News channel aesthetic (like CNN, BBC, Fox News style thumbnails)
- Visual elements: Use icons, symbols, charts, or stylized graphics - avoid photorealistic politicians
- Typography: Bold sans-serif font, white or yellow text with dark outline for maximum readability
- Composition: Strong focal point, rule of thirds, visual hierarchy that guides the eye

TECHNICAL:
- High resolution, sharp details
- No watermarks or logos
- Professional lighting and shadows
- Modern, clean design aesthetic

Make it MAXIMUM CLICK-THROUGH RATE - this thumbnail needs to stand out in YouTube search results!
`.trim();

  // Track which provider we used for caching
  let usedProvider = 'unknown';

  // Helper function to generate a single thumbnail with WaveSpeed + DALL-E fallback
  const generateSingleThumbnail = async (prompt: string): Promise<string | null> => {
    // Try WaveSpeed generation model first (no placeholder)
    if (isWavespeedConfigured()) {
      try {
        console.log(`üé® [Thumbnail Variant] Trying WaveSpeed generation model...`);
        const taskId = await createWavespeedImageTask(prompt, '16:9');
        const imageUrl = await pollWavespeedImageTask(taskId);
        const dataUri = await imageUrlToDataUri(imageUrl);
        CostTracker.track('thumbnail', 'wavespeed/nano-banana-pro', 0.14);
        console.log(`‚úÖ [Thumbnail Variant] Generated via WaveSpeed`);
        usedProvider = 'wavespeed';
        return dataUri;
      } catch (genError) {
        console.warn(`‚ö†Ô∏è WaveSpeed generation failed, trying edit model:`, (genError as Error).message);
        
        // Fallback to edit model with placeholder
        try {
          console.log(`üé® [Thumbnail Variant] Trying WaveSpeed edit model...`);
          const placeholderImage = createPlaceholderImage(1024, 576);
          const taskId = await createWavespeedImageTask(prompt, '16:9', placeholderImage);
          const imageUrl = await pollWavespeedImageTask(taskId);
          const dataUri = await imageUrlToDataUri(imageUrl);
          CostTracker.track('thumbnail', 'wavespeed/nano-banana-pro', 0.14);
          console.log(`‚úÖ [Thumbnail Variant] Generated via WaveSpeed (edit)`);
          usedProvider = 'wavespeed';
          return dataUri;
        } catch (editError) {
          console.warn(`‚ö†Ô∏è WaveSpeed edit model also failed, trying DALL-E:`, (editError as Error).message);
        }
      }
    }

    // Enhanced DALL-E fallback with better prompt
    try {
      console.log(`üé® [Thumbnail Variant] Using DALL-E 3 with enhanced prompt...`);
      const dalleImage = await generateImageWithDALLE(prompt, '1792x1024');
      if (dalleImage) {
        console.log(`‚úÖ [Thumbnail Variant] Generated via DALL-E 3`);
        usedProvider = 'dalle';
      }
      return dalleImage;
    } catch (e) {
      console.error(`‚ùå DALL-E thumbnail failed:`, (e as Error).message);
      return null;
    }
  };


  console.log(`üé® [Thumbnails] Generating ${variantCount} thumbnail variants for A/B testing...`);

  try {
    // Select styles to use (diverse selection)
    const selectedStyles = styles.slice(0, Math.min(variantCount, styles.length));
    
    // Generate all variants in parallel (but limit concurrency to avoid rate limits)
    const batchSize = 3; // Generate 3 at a time
    const allVariants: Array<{ url: string | null; style: string }> = [];
    
    for (let i = 0; i < selectedStyles.length; i += batchSize) {
      const batch = selectedStyles.slice(i, i + batchSize);
      const batchResults = await Promise.all(
        batch.map(style => 
          generateSingleThumbnail(basePrompt(style))
            .then(url => ({ url, style: style.name }))
            .catch(() => ({ url: null, style: style.name }))
        )
      );
      allVariants.push(...batchResults);
    }

    // Analyze all variants
    const analyzedVariants = allVariants
      .filter(v => v.url !== null)
      .map(v => ({
        url: v.url!,
        analysis: analyzeThumbnail(v.url, v.style)
      }))
      .sort((a, b) => b.analysis.predictedCTR - a.analysis.predictedCTR); // Sort by predicted CTR

    console.log(`‚úÖ [Thumbnails] Generated ${analyzedVariants.length} variants, best CTR: ${analyzedVariants[0]?.analysis.predictedCTR}%`);
    
    // Save best variants to cache
    if (channelId && analyzedVariants.length > 0) {
      const best = analyzedVariants[0];
      const secondBest = analyzedVariants[1];
      await saveThumbnailToCache(
        channelId,
        productionId || null,
        newsContext,
        viralMeta.title,
        best.url,
        secondBest?.url,
        best.analysis.style,
        usedProvider
      );
    }

    return {
      variants: analyzedVariants,
      best: analyzedVariants[0] || null
    };
  } catch (e) {
    console.error("Thumbnail variants generation failed", e);
    const fallback = await generateThumbnail(newsContext, config);
    if (fallback) {
      const fallbackAnalysis = analyzeThumbnail(fallback, 'fallback');
      return {
        variants: [{ url: fallback, analysis: fallbackAnalysis }],
        best: { url: fallback, analysis: fallbackAnalysis }
      };
    }
    return { variants: [], best: null };
  }
};

/**
 * Legacy wrapper for generateThumbnailVariants - maintains backward compatibility
 * Returns { primary, variant } format while using new advanced generation
 */
export const generateThumbnailVariants = async (
  newsContext: string,
  config: ChannelConfig,
  viralMeta: ViralMetadata,
  channelId?: string,
  productionId?: string
): Promise<{ primary: string | null; variant: string | null }> => {
  const result = await generateThumbnailVariantsAdvanced(
    newsContext,
    config,
    viralMeta,
    channelId,
    productionId,
    8 // Generate 8 variants
  );
  
  // Return in legacy format for compatibility
  return {
    primary: result.best?.url || null,
    variant: result.variants[1]?.url || null
  };
};

export const generateViralHook = async (
  news: NewsItem[],
  config: ChannelConfig
): Promise<string> => {
  // Use GPT-4o for viral hook (no more Gemini quota issues!)
  console.log(`üé£ [Hook] Generating viral hook using GPT-4o...`);
  
  try {
    const hook = await generateViralHookWithGPT(news, config);
    console.log(`‚úÖ [Hook] Successfully generated hook via GPT-4o`);
    return hook;
  } catch (error) {
    console.error(`‚ùå [Hook] GPT-4o failed:`, (error as Error).message);
    return `Breaking news about ${news[0]?.headline?.substring(0, 30) || 'today'}...`;
  }
};

// =============================================================================================
// VIDEO COMPOSITION (Shotstack - Cloud FFmpeg)
// =============================================================================================

/**
 * Check if video composition is available
 */
export const isCompositionAvailable = (): boolean => {
  const config = checkShotstackConfig();
  return config.configured;
};

/**
 * Compose final video from segments using Shotstack
 * 
 * Uses PODCAST-STYLE composition (following shockstack.md guide):
 * - Videos play sequentially (NO overlaps)
 * - Minimalist lower thirds with titles
 * - Subtle frame/border
 * - Soft vignette overlay
 * - Fade transitions only
 * - NO separate audio track (videos have embedded audio from InfiniteTalk)
 * 
 * @param segments - Broadcast segments with video URLs
 * @param videoUrls - Array of video URLs (from InfiniteTalk)
 * @param videos - Video assets (intro/outro) - currently not used in podcast style
 * @param config - Channel config
 * @param options - Composition options
 */
export const composeVideoWithShotstack = async (
  segments: BroadcastSegment[],
  videoUrls: (string | null)[],
  videos: VideoAssets,
  config: ChannelConfig,
  options: {
    resolution?: '1080' | 'hd' | 'sd';
    transition?: 'fade' | 'wipeLeft' | 'slideLeft' | 'slideRight' | 'zoom';
    transitionDuration?: number;
    watermarkUrl?: string;
    callbackUrl?: string;
    enableOverlays?: boolean;
    breakingNewsTitle?: string;
    headlines?: string[];
    // Scene titles for lower thirds (optional, will use headlines if not provided)
    sceneTitles?: string[];
  } = {}
): Promise<RenderResult> => {
  // Check if Shotstack is configured
  const shotstackConfig = checkShotstackConfig();
  if (!shotstackConfig.configured) {
    console.warn('‚ö†Ô∏è [Composition] Shotstack not configured');
    console.warn('‚ö†Ô∏è [Composition] Set VITE_SHOTSTACK_API_KEY to enable video composition');
    return {
      success: false,
      error: 'Shotstack not configured. Set VITE_SHOTSTACK_API_KEY in your environment.'
    };
  }

  // Filter out null video URLs and build scenes
  const validSegments: { segment: BroadcastSegment; videoUrl: string; index: number }[] = [];
  segments.forEach((segment, index) => {
    const videoUrl = videoUrls[index] || segment.videoUrl;
    if (videoUrl) {
      validSegments.push({ segment, videoUrl, index });
    }
  });

  if (validSegments.length === 0) {
    console.error('‚ùå [Composition] No valid video URLs to compose');
    return {
      success: false,
      error: 'No valid video URLs to compose'
    };
  }

  console.log(`üéôÔ∏è [Podcast Composition] Starting podcast-style video composition...`);
  console.log(`üéôÔ∏è [Podcast Composition] ${validSegments.length}/${segments.length} valid video segments`);
  console.log(`üéôÔ∏è [Podcast Composition] Style: Clean podcast aesthetic (no TV news overlays)`);
  console.log(`üéôÔ∏è [Podcast Composition] NOTE: Videos play SEQUENTIALLY (no overlaps)`);

  try {
    // Convert segments to PodcastScene format
    // CRITICAL: Each scene needs a duration so we can calculate start times
    const podcastScenes: PodcastScene[] = validSegments.map(({ segment, videoUrl, index }) => {
      // Use actual audio duration from segment (set during audio generation)
      // This is the REAL duration, not an estimate
      const audioDuration = segment.audioDuration;
      
      // Fallback: Estimate duration from text length (only if audioDuration not available)
      let duration: number;
      if (audioDuration && audioDuration > 0) {
        duration = audioDuration;
        console.log(`üéôÔ∏è [Podcast Scene ${index}] Using REAL duration: ${duration.toFixed(1)}s`);
      } else {
        // Estimate: ~150 words/minute = 2.5 words/sec
        const wordCount = segment.text.split(/\s+/).length;
        duration = Math.max(3, Math.ceil(wordCount / 2.5));
        console.log(`üéôÔ∏è [Podcast Scene ${index}] Using ESTIMATED duration: ${duration.toFixed(1)}s (no audioDuration available)`);
      }

      // Get title for lower third
      // Priority: segment.sceneTitle (from Supabase/Narrative Engine) > sceneTitles > headlines > text
      let title = '';
      if (segment.sceneTitle) {
        // Best: Use title generated by Narrative Engine and stored in Supabase
        title = segment.sceneTitle;
        console.log(`üéôÔ∏è [Podcast Scene ${index}] Using scene title from Supabase: "${title}"`);
      } else if (options.sceneTitles && options.sceneTitles[index]) {
        title = options.sceneTitles[index];
      } else if (options.headlines && options.headlines[index]) {
        title = options.headlines[index];
      } else {
        // Fallback: Use first 50 chars of segment text as title
        title = segment.text.length > 50 
          ? segment.text.substring(0, 47) + '...'
          : segment.text;
        console.log(`üéôÔ∏è [Podcast Scene ${index}] No sceneTitle found, using text fallback`);
      }

      // Get speaker name
      const speakerName = segment.speaker === 'host_a' 
        ? (config.characters?.hostA?.name || 'Host A')
        : segment.speaker === 'host_b'
        ? (config.characters?.hostB?.name || 'Host B')
        : segment.speaker;

      return {
        video_url: videoUrl,
        title,
        duration,
        speaker: speakerName
      };
    });

    console.log(`üéôÔ∏è [Podcast Composition] Built ${podcastScenes.length} scenes:`);
    let totalDuration = 0;
    podcastScenes.forEach((scene, i) => {
      console.log(`  Scene ${i + 1}: ${scene.duration}s - "${scene.title.substring(0, 30)}..." (${scene.speaker})`);
      totalDuration += scene.duration;
    });
    console.log(`üéôÔ∏è [Podcast Composition] Estimated total duration: ${totalDuration}s`);

    // Render using podcast-style composition
    const result = await renderPodcastVideo(podcastScenes, {
      channelName: config.channelName,
      episodeTitle: options.breakingNewsTitle,
      showBorder: true,
      showVignette: true,
      resolution: options.resolution || '1080'
    });

    if (result.success) {
      console.log(`‚úÖ [Podcast Composition] Video composed successfully!`);
      console.log(`üé• [Podcast Composition] URL: ${result.videoUrl}`);
      console.log(`üñºÔ∏è [Podcast Composition] Poster: ${result.posterUrl}`);
      console.log(`‚è±Ô∏è [Podcast Composition] Duration: ${result.duration}s`);
      console.log(`üí∞ [Podcast Composition] Cost: $${result.cost?.toFixed(4) || '?'}`);
    } else {
      console.error(`‚ùå [Podcast Composition] Failed: ${result.error}`);
    }

    return result;
  } catch (error) {
    console.error('‚ùå [Podcast Composition] Error:', (error as Error).message);
    return {
      success: false,
      error: (error as Error).message
    };
  }
};

/**
 * Quick composition check - returns what's available
 */
export const getCompositionStatus = () => {
  const shotstack = checkShotstackConfig();
  
  return {
    shotstack: {
      available: shotstack.configured,
      message: shotstack.message,
      pricing: '~$0.05/min of video'
    },
    recommendation: shotstack.configured 
      ? 'Use Shotstack for professional video composition'
      : 'Configure Shotstack API key for video composition'
  };
};

// Re-export script analysis for YouTube Shorts
export { analyzeScriptForShorts };
export type { ScriptAnalysis };

// Simplified scene regeneration function for ProductionWizard
// This is a wrapper around the more complex regenerateScene from openaiService
export const regenerateScene = async (
  currentScene: { title?: string; text: string; video_mode: string; shot?: string; model?: string },
  prevSceneText: string | null,
  nextSceneText: string | null,
  hostAName: string,
  hostBName: string,
  language: string
): Promise<{ title: string; text: string } | null> => {
  const speaker = currentScene.video_mode === 'hostA' ? hostAName : hostBName;
  
  // Language handling
  const isSpanish = (language || '').toLowerCase().includes('spanish') || 
                    (language || '').toLowerCase().includes('espa√±ol');
  
  const languageInstruction = isSpanish
    ? `IMPORTANTE: Genera el contenido COMPLETAMENTE en ESPA√ëOL.`
    : `Generate content in English.`;
  
  const systemPrompt = `You are a scriptwriter for a news podcast. You need to regenerate a single scene.

${languageInstruction}

SPEAKER: ${speaker}
STYLE: Conversational podcast banter

The scene should:
- Be 40-80 words
- Match the speaker's personality
- Flow naturally from the previous scene and into the next
- Keep the podcast banter style

Return ONLY valid JSON:
{
  "title": "Short catchy scene title (3-6 words)",
  "text": "The regenerated dialogue (40-80 words)"
}`;

  const userPrompt = `PREVIOUS SCENE:
${prevSceneText || '(This is the first scene)'}

CURRENT SCENE TO REGENERATE:
Title: ${currentScene.title || 'Scene'}
Original text: ${currentScene.text}

NEXT SCENE:
${nextSceneText || '(This is the last scene)'}

Please regenerate this scene with fresh dialogue that improves on the original.`;

  try {
    const response = await openaiRequest('chat/completions', {
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: systemPrompt },
        { role: 'user', content: userPrompt }
      ],
      response_format: { type: 'json_object' },
      temperature: 0.8
    }, { timeout: 20000 });

    CostTracker.track('scene_regenerate', 'gpt-4o-mini', 0.002);

    const result = JSON.parse(response.choices[0].message.content);
    
    return {
      title: result.title || currentScene.title || 'Scene',
      text: result.text || currentScene.text
    };
  } catch (error) {
    console.error(`[Scene Regen] Failed to regenerate scene:`, error);
    return null;
  }
};
